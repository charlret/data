{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import zstandard as zstd\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de parties valides détectées : 500000\n"
     ]
    }
   ],
   "source": [
    "def parse_and_filter_txt(txt_path, required_fields=None, max_games=1500000):\n",
    "    \"\"\"\n",
    "    Parse un fichier TXT en un DataFrame Pandas avec filtrage des champs obligatoires et limite de parties.\n",
    "\n",
    "    :param txt_path: Chemin du fichier TXT.\n",
    "    :param required_fields: Liste des champs requis pour garder une partie.\n",
    "    :param max_games: Nombre maximum de parties à ajouter au DataFrame.\n",
    "    :return: DataFrame Pandas contenant les parties valides.\n",
    "    \"\"\"\n",
    "    if required_fields is None:\n",
    "        required_fields = ['WhiteElo', 'BlackElo', 'Opening', 'TimeControl', 'Termination']\n",
    "\n",
    "    game_data = []  # Liste pour stocker les parties valides\n",
    "\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "        txt_content = f.read()\n",
    "\n",
    "    # Séparer les parties en utilisant une ligne vide comme séparateur\n",
    "    games = txt_content.strip().split(\"\\n\\n\")\n",
    "\n",
    "    for game in games:\n",
    "        # Diviser les métadonnées et les coups\n",
    "        lines = game.split(\"\\n\")\n",
    "        metadata_lines = [line for line in lines if line.startswith(\"[\")]\n",
    "        moves_lines = [line for line in lines if not line.startswith(\"[\")]\n",
    "\n",
    "        # Extraire les métadonnées\n",
    "        metadata = dict(re.findall(r'\\[(\\w+) \"(.*?)\"\\]', \"\\n\".join(metadata_lines)))\n",
    "\n",
    "        # Vérifier que tous les champs obligatoires sont présents\n",
    "        if all(field in metadata for field in required_fields):\n",
    "            # Extraire les coups\n",
    "            moves = \" \".join(moves_lines).replace('\\n', ' ')  # Joindre les coups en une seule chaîne\n",
    "\n",
    "            # Ajouter la partie si elle satisfait les critères\n",
    "            metadata['Moves'] = moves\n",
    "            game_data.append(metadata)\n",
    "\n",
    "        # Vérifier si nous avons atteint le nombre maximal de parties\n",
    "        if len(game_data) >= max_games:\n",
    "            break\n",
    "\n",
    "    # Créer un DataFrame à partir des parties valides\n",
    "    df = pd.DataFrame(game_data)\n",
    "    return df\n",
    "\n",
    "# Exemple d'utilisation\n",
    "txt_file = \"/home/onyxia/work/data/Data.txt\"\n",
    "required_fields = ['WhiteElo', 'BlackElo', 'Opening', 'TimeControl', 'Termination']\n",
    "df = parse_and_filter_txt(txt_file, required_fields=required_fields, max_games=500000)\n",
    "\n",
    "\n",
    "\n",
    "# Sauvegarder correctement le DataFrame en CSV\n",
    "output_csv = \"Data_uncleaned.csv\"\n",
    "df.to_csv(output_csv, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 18)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_filter_pgn(pgn_path, required_fields=None):\n",
    "    \"\"\"\n",
    "    Parse un fichier PGN en un DataFrame Pandas avec gestion des erreurs et filtrage des champs obligatoires.\n",
    "\n",
    "    :param pgn_path: Chemin du fichier PGN.\n",
    "    :param required_fields: Liste des champs requis pour garder une partie.\n",
    "    :return: DataFrame Pandas contenant les parties valides.\n",
    "    \"\"\"\n",
    "    if required_fields is None:\n",
    "        required_fields = ['WhiteElo', 'BlackElo', 'Opening', 'TimeControl', 'Termination']\n",
    "\n",
    "    game_data = []  # Liste pour stocker les parties valides\n",
    "    invalid_games = []  # Parties non valides pour diagnostic\n",
    "\n",
    "    with open(pgn_path, 'r', encoding='utf-8') as f:\n",
    "        pgn_content = f.read()\n",
    "\n",
    "    # Séparer les parties en utilisant [Event \"...\"] comme séparateur\n",
    "    games = re.split(r'\\n(?=\\[Event\\s\")', pgn_content.strip())\n",
    "\n",
    "    for i, game in enumerate(games):\n",
    "        try:\n",
    "            # Extraire les métadonnées\n",
    "            metadata = dict(re.findall(r'\\[(\\w+) \"(.*?)\"\\]', game))\n",
    "\n",
    "            # Vérifier que tous les champs obligatoires sont présents\n",
    "            missing_fields = [field for field in required_fields if field not in metadata]\n",
    "            if missing_fields:\n",
    "                raise ValueError(f\"Champs obligatoires manquants : {', '.join(missing_fields)}\")\n",
    "\n",
    "            # Extraire les mouvements (facultatif)\n",
    "            moves = re.search(r'(1\\..+)', game, re.DOTALL)\n",
    "            metadata['Moves'] = moves.group(1).replace('\\n', ' ') if moves else ''\n",
    "\n",
    "            # Ajouter la partie si elle satisfait les critères\n",
    "            game_data.append(metadata)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Enregistrer la partie non valide pour diagnostic\n",
    "            invalid_games.append((i + 1, str(e), game))\n",
    "\n",
    "    # Créer un DataFrame à partir des parties valides\n",
    "    df = pd.DataFrame(game_data)\n",
    "\n",
    "    # Afficher les erreurs détectées\n",
    "    if invalid_games:\n",
    "        print(f\"{len(invalid_games)} parties ont été ignorées en raison d'erreurs ou de champs manquants.\")\n",
    "        for idx, (game_no, error, game_content) in enumerate(invalid_games[:5]):  # Limiter à 5 erreurs\n",
    "            print(f\"Partie {game_no} ignorée : {error}\\nContenu :\\n{game_content}\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Exemple d'utilisation\n",
    "pgn_file = \"/home/onyxia/work/data/output.txt\"\n",
    "required_fields = ['WhiteElo', 'BlackElo', 'Opening', 'TimeControl', 'Termination']\n",
    "df = parse_and_filter_pgn(pgn_file, required_fields=required_fields)\n",
    "\n",
    "# Assurez-vous que le DataFrame est bien créé\n",
    "print(f\"Nombre de parties valides détectées : {len(df)}\")\n",
    "\n",
    "# Sauvegarder correctement le DataFrame en CSV\n",
    "output_csv = \"Data_uncleaned.csv\"\n",
    "df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "print(f\"Fichier CSV écrit avec succès : {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import os\n",
    "\n",
    "# Chemin du fichier CSV local\n",
    "local_file_path = \"/home/onyxia/work/data/Data_uncleaned.csv\"\n",
    "local_file_path2 = \"/home/onyxia/work/data/output.txt\"\n",
    "# Configuration du bucket S3 et du chemin de destination\n",
    "MY_BUCKET = \"charlret\"\n",
    "FILE_PATH_OUT_S3 = f\"{MY_BUCKET}/diffusion/Data.csv\"\n",
    "FILE_PATH_OUT_S3_2 = f\"{MY_BUCKET}/diffusion/texte.txt\"\n",
    "\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = 'FVWK0NHMH7H731QHWURF'\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = 'RJ5oGkNswBPVxH76iVt2dwizr5+A7HiKEANj0lC7'\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = 'eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiJGVldLME5ITUg3SDczMVFIV1VSRiIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNzM0NTEyMjk5LCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6ImNoYXJsZXMuZGVyYWluQGVuc2FlLmZyIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImV4cCI6MTczNTkwNDk5MCwiZmFtaWx5X25hbWUiOiJEZXJhaW4iLCJnaXZlbl9uYW1lIjoiQ2hhcmxlcyIsImdyb3VwcyI6WyJVU0VSX09OWVhJQSJdLCJpYXQiOjE3MzUzMDAxODgsImlzcyI6Imh0dHBzOi8vYXV0aC5sYWIuc3NwY2xvdWQuZnIvYXV0aC9yZWFsbXMvc3NwY2xvdWQiLCJqdGkiOiIyYTgxNWU1Ny0zZDEyLTRjOGQtOGM2My04Njk3NWU1Nzc3YjIiLCJuYW1lIjoiQ2hhcmxlcyBEZXJhaW4iLCJwb2xpY3kiOiJzdHNvbmx5IiwicHJlZmVycmVkX3VzZXJuYW1lIjoiY2hhcmxyZXQiLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiIsImRlZmF1bHQtcm9sZXMtc3NwY2xvdWQiXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJyb2xlcyI6WyJvZmZsaW5lX2FjY2VzcyIsInVtYV9hdXRob3JpemF0aW9uIiwiZGVmYXVsdC1yb2xlcy1zc3BjbG91ZCJdLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGdyb3VwcyBlbWFpbCIsInNpZCI6ImRmYzMwNjZkLWFjZjEtNDFmZi04ODU2LWUzNjlkMzFiZjk5OCIsInN1YiI6Ijg3MGIwZWRjLTE2ZGQtNDRmOS1hMDRjLWMyNzNiMzcyOGYyNiIsInR5cCI6IkJlYXJlciJ9.fZwXn4U5ak5NCLCz1wZcPNAqwsYcw7GqGavNIVEMUvtEGeg2uIkoO4Zv7COnNtkWsprn0KtGgj9EU7tMoSbqGA'\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = 'us-east-1'\n",
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n",
    "    key = os.environ[\"AWS_ACCESS_KEY_ID\"], \n",
    "    secret = os.environ[\"AWS_SECRET_ACCESS_KEY\"], \n",
    "    token = os.environ[\"AWS_SESSION_TOKEN\"])\n",
    "\n",
    "# Ouvrir et envoyer le fichier CSV local vers S3\n",
    "with open(local_file_path, \"rb\") as local_file:\n",
    "    with fs.open(FILE_PATH_OUT_S3, \"wb\") as s3_file:\n",
    "        s3_file.write(local_file.read())\n",
    "\n",
    "with open(local_file_path2, \"rb\") as local_file:\n",
    "    with fs.open(FILE_PATH_OUT_S3_2, \"wb\") as s3_file:\n",
    "        s3_file.write(local_file.read())\n",
    "\n",
    "print(f\"Fichier '{local_file_path}' a été sauvegardé dans S3 sous '{FILE_PATH_OUT_S3}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
